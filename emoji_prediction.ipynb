{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Prediction Classifier\n",
    "\n",
    "<img src=\"data_set.png\" style=\"width:100px;height:100px;\">\n",
    "<caption><center> **Figure 1**: EMOJISET - a classification problem with 5 classes. A few examples of sentences are given here. </center></caption>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>French macaroon is so tasty</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>work is horrible</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am upset</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>throw the ball</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good joke</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Sentence  label  Unnamed: 2 Unnamed: 3\n",
       "0  French macaroon is so tasty      4         NaN        NaN\n",
       "1             work is horrible      3         NaN        NaN\n",
       "2                   I am upset      3         NaN        [3]\n",
       "3               throw the ball      1         NaN        [2]\n",
       "4                    Good joke      2         NaN        NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(\"emojify_data.csv\" )\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to eat</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he did not answer</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got a raise</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she got me a present</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha ha ha it was so funny</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Sentence  label\n",
       "0             I want to eat      4\n",
       "1         he did not answer      3\n",
       "2            he got a raise      2\n",
       "3      she got me a present      0\n",
       "4  ha ha ha it was so funny      2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=pd.read_csv(\"test_emoji.csv\" )\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['Sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "labels = []\n",
    "raw_sents = []\n",
    "for i in range(len(data['Sentence'])):\n",
    "    line = data['Sentence'][i].strip()\n",
    "    words = line.split(' ')\n",
    "    raw_sents.append(words)\n",
    "    labels.append(int(data['label'][i]))\n",
    "    for word in words:\n",
    "        vocab.add(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw_sents=[]\n",
    "test_clabels=[]\n",
    "for i in range(len(test_data['Sentence'])):\n",
    "    tline = test_data['Sentence'][i].strip()\n",
    "    twords = tline.split(' ')\n",
    "    test_raw_sents.append(twords)\n",
    "    test_clabels.append(int(test_data['label'][i]))\n",
    "    for tword in twords:\n",
    "        vocab.add(tword.lower())\n",
    "\n",
    "vocab = ['<PAD>'] + sorted(list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_sents\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_sents = []\n",
    "for cur_raw_sent in raw_sents:\n",
    "    cur_indexed_sent = []\n",
    "    for word in cur_raw_sent:\n",
    "        cur_indexed_sent.append(vocab.index(word.lower()))\n",
    "    indexed_sents.append(cur_indexed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[111, 186, 160, 246, 267],\n",
       " [304, 160, 148],\n",
       " [154, 13, 284],\n",
       " [275, 270, 34],\n",
       " [124, 165]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_sents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indexed_sents = []\n",
    "for tcur_raw_sent in test_raw_sents:\n",
    "    tcur_indexed_sent = []\n",
    "    for word in tcur_raw_sent:\n",
    "        tcur_indexed_sent.append(vocab.index(word.lower()))\n",
    "    test_indexed_sents.append(tcur_indexed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = -1\n",
    "for cur_indexed_sent in indexed_sents:\n",
    "    max_len = max(max_len, len(cur_indexed_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sents = []\n",
    "for cur_indexed_sent in indexed_sents:\n",
    "    cur_padded_sent=[]\n",
    "    for x in range(max_len-len(cur_indexed_sent)):\n",
    "        cur_padded_sent.append(0)\n",
    "    for words in cur_indexed_sent:\n",
    "        cur_padded_sent.append(words)\n",
    "    padded_sents.append(cur_padded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 0, 111, 186, 160, 246, 267],\n",
       " [0, 0, 0, 0, 0, 0, 0, 304, 160, 148],\n",
       " [0, 0, 0, 0, 0, 0, 0, 154, 13, 284],\n",
       " [0, 0, 0, 0, 0, 0, 0, 275, 270, 34],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 124, 165]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_sents[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmax_len = -1\n",
    "for tcur_indexed_sent in test_indexed_sents:\n",
    "    tmax_len = max(tmax_len, len(tcur_indexed_sent))\n",
    "tmax_len=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded_sents = []\n",
    "for tcur_indexed_sent in test_indexed_sents:\n",
    "    tcur_padded_sent=[]\n",
    "    for x in range(tmax_len-len(tcur_indexed_sent)):\n",
    "        tcur_padded_sent.append(0)\n",
    "    for words in tcur_indexed_sent:\n",
    "        tcur_padded_sent.append(words)\n",
    "    test_padded_sents.append(tcur_padded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "183\n",
      "183\n",
      "56\n",
      "56\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_sents))\n",
    "print(len(indexed_sents))\n",
    "print(len(padded_sents))\n",
    "print(len(test_raw_sents))\n",
    "print(len(test_indexed_sents))\n",
    "print(len(test_padded_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_sents[0]))\n",
    "print(len(indexed_sents[0]))\n",
    "print(len(padded_sents[0]))\n",
    "X_test=np.array(test_padded_sents)\n",
    "Y_test=np.array(test_clabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(padded_sents)\n",
    "Y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(183,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "Y = torch.from_numpy(Y).long()\n",
    "X_test = torch.from_numpy(X_test)\n",
    "Y_test = torch.from_numpy(Y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = X\n",
    "trainY = Y\n",
    "testX = X_test\n",
    "testY = Y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 10])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split(' ')\n",
    "    word = values[0] ## The first entry is the word\n",
    "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.092086,  0.2571  , -0.58693 , -0.37029 ,  1.0828  , -0.55466 ,\n",
       "       -0.78142 ,  0.58696 , -0.58714 ,  0.46318 , -0.11267 ,  0.2606  ,\n",
       "       -0.26928 , -0.072466,  1.247   ,  0.30571 ,  0.56731 ,  0.30509 ,\n",
       "       -0.050312, -0.64443 , -0.54513 ,  0.86429 ,  0.20914 ,  0.56334 ,\n",
       "        1.1228  , -1.0516  , -0.78105 ,  0.29656 ,  0.7261  , -0.61392 ,\n",
       "        2.4225  ,  1.0142  , -0.17753 ,  0.4147  , -0.12966 , -0.47064 ,\n",
       "        0.3807  ,  0.16309 , -0.323   , -0.77899 , -0.42473 , -0.30826 ,\n",
       "       -0.42242 ,  0.055069,  0.38267 ,  0.037415, -0.4302  , -0.39442 ,\n",
       "        0.10511 ,  0.87286 ], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unk_vector = embeddings_index.vectors.mean(axis=0)\n",
    "avg=np.zeros((50,1),dtype=float)\n",
    "sum1=0\n",
    "for word in vocab[1:]:\n",
    "    sum1=sum1+embeddings_index[word]\n",
    "avg=sum1/len(vocab)\n",
    "avg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10477203,  0.10030416, -0.11105632, -0.21022114,  0.43222502,\n",
       "       -0.01908446, -0.4056496 ,  0.12348095, -0.21135695,  0.1374001 ,\n",
       "       -0.15094624,  0.14368933, -0.18328223,  0.04002608,  0.5344314 ,\n",
       "        0.11114672,  0.10141597,  0.09200446, -0.16404863, -0.40154225,\n",
       "       -0.07510382,  0.32814342,  0.2774701 ,  0.12376319,  0.41074434,\n",
       "       -1.270989  , -0.38609952,  0.1566317 ,  0.43546626, -0.3857825 ,\n",
       "        2.4428022 ,  0.40749273, -0.21845146, -0.0455374 , -0.0095862 ,\n",
       "        0.01084257,  0.07582554,  0.21608587,  0.01072802, -0.31934437,\n",
       "       -0.15144747,  0.05009922, -0.10021205,  0.18748248,  0.12095157,\n",
       "        0.06794936, -0.09993503, -0.10877771,  0.00478268,  0.22292644],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [np.zeros((50),dtype=float)]\n",
    "for word in vocab[1:]:\n",
    "    if word in embeddings_index.keys():\n",
    "        vectors.append(embeddings_index[word])\n",
    "    else:\n",
    "        print('unk')\n",
    "        vectors.append(unk_vector)\n",
    "\n",
    "vectors = np.array(vectors, dtype=np.float32)\n",
    "vectors = torch.from_numpy(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([314, 50])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_seq_len, embed_dim, hidden_dim, output_dim):\n",
    "        super(Network, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedder = nn.Embedding(len(vocab), self.embed_dim)\n",
    "        self.embedder.weight.data.copy_(vectors)\n",
    "        self.fc1 = nn.Linear(self.max_seq_len * self.embed_dim, self.hidden_dim)\n",
    "        self.soft1 = nn.Softmax()\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def embed(self, x):\n",
    "        return self.embedder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedder(x)\n",
    "#         print(x.size())\n",
    "        x = x.view(-1, self.max_seq_len * self.embed_dim)\n",
    "#         print(x.size())\n",
    "        x = self.fc1(x)\n",
    "        x = self.soft1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (embedder): Embedding(314, 50)\n",
       "  (fc1): Linear(in_features=500, out_features=200, bias=True)\n",
       "  (soft1): Softmax()\n",
       "  (fc2): Linear(in_features=200, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network(max_len, 50, 200,5 )\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtinterndec18/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  loss =  1.6189097166061401\n",
      "Epoch  10  loss =  1.60488760471344\n",
      "Epoch  20  loss =  1.5914041996002197\n",
      "Epoch  30  loss =  1.576784372329712\n",
      "Epoch  40  loss =  1.558550238609314\n",
      "Epoch  50  loss =  1.5351389646530151\n",
      "Epoch  60  loss =  1.5065057277679443\n",
      "Epoch  70  loss =  1.4743640422821045\n",
      "Epoch  80  loss =  1.4404727220535278\n",
      "Epoch  90  loss =  1.4062623977661133\n",
      "Epoch  100  loss =  1.373213529586792\n",
      "Epoch  110  loss =  1.3415652513504028\n",
      "Epoch  120  loss =  1.3111374378204346\n",
      "Epoch  130  loss =  1.281215786933899\n",
      "Epoch  140  loss =  1.2516783475875854\n",
      "Epoch  150  loss =  1.2229323387145996\n",
      "Epoch  160  loss =  1.1956560611724854\n",
      "Epoch  170  loss =  1.1694023609161377\n",
      "Epoch  180  loss =  1.1438428163528442\n",
      "Epoch  190  loss =  1.1187708377838135\n",
      "Epoch  200  loss =  1.0956748723983765\n",
      "Epoch  210  loss =  1.0731490850448608\n",
      "Epoch  220  loss =  1.0524749755859375\n",
      "Epoch  230  loss =  1.0332132577896118\n",
      "Epoch  240  loss =  1.0148608684539795\n",
      "Epoch  250  loss =  0.9972827434539795\n",
      "Epoch  260  loss =  0.98039710521698\n",
      "Epoch  270  loss =  0.9641289114952087\n",
      "Epoch  280  loss =  0.9484162330627441\n",
      "Epoch  290  loss =  0.9332101345062256\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch_cntr in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    trainO = model(trainX)\n",
    "    loss = criterion(trainO, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch_cntr % 10 == 0:\n",
    "        print('Epoch ', epoch_cntr, ' loss = ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtinterndec18/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.994535519125683"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train accuracy\n",
    "trainO = model(trainX)\n",
    "trainP = torch.argmax(trainO, dim=1)\n",
    "(trainY == trainP).sum().item() / trainY.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtinterndec18/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9821428571428571"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testO = model(testX)\n",
    "testP = torch.argmax(testO, dim=1)\n",
    "\n",
    "(testY == testP).sum().item() / testY.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sent(sent):\n",
    "    sent = sent.strip()\n",
    "    words = sent.split(' ')\n",
    "    padded_sents = []\n",
    "    indexed_sent = []\n",
    "    for word in words:\n",
    "        indexed_sent.append(vocab.index(word.lower()))\n",
    "    indexed_len = len(indexed_sent)\n",
    "    for x in range(max_len-len(indexed_sent)):\n",
    "        padded_sents.append(0)\n",
    "    for word in indexed_sent:\n",
    "        padded_sents.append(word)\n",
    "    x = np.array(padded_sents)\n",
    "    x = torch.from_numpy(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   0,   0,   0, 154,  13, 135,  51, 312, 304])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent = encode_sent('i am happy by your work')\n",
    "test_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtinterndec18/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sent_output = model(test_sent)\n",
    "test_sent_pred = torch.argmax(test_sent_output, dim=1)\n",
    "test_sent_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_seq_len, embed_dim, hidden_dim, hidden_state_dim, output_dim):\n",
    "        super(RecurrentNetwork, self).__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_state_dim = hidden_state_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedder = nn.Embedding(len(vocab), self.embed_dim)\n",
    "        self.embedder.weight.data.copy_(vectors)\n",
    "        \n",
    "        self.U = nn.Linear(self.hidden_state_dim, self.hidden_state_dim)\n",
    "        self.W = nn.Linear(self.embed_dim, self.hidden_state_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(self.hidden_state_dim, self.hidden_dim)\n",
    "        self.fc2 = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        \n",
    "    def embed(self, x):\n",
    "        return self.embedder(x)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.embedder(x)\n",
    "        h = torch.zeros(x.size(0), self.hidden_state_dim)\n",
    "#         print(x.size(), self.W.weight.shape)\n",
    "#         print(h.size(), self.U.weight.shape)\n",
    "        for i in range(x.size(1)):\n",
    "#             print(\"h shape:-\",h.shape)\n",
    "#             print(\"x shape\",x.shape)\n",
    "            a = self.U(h)\n",
    "            b = self.W(x[:,i])\n",
    "#             print(a.shape, b.shape)\n",
    "            h = torch.nn.functional.relu(self.U(h) + self.W(x[:,i]))\n",
    "        h = self.fc1(h)\n",
    "        h = self.relu2(h)\n",
    "        h = self.fc2(h)\n",
    "    \n",
    "        return h\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecurrentNetwork(\n",
       "  (embedder): Embedding(314, 50)\n",
       "  (U): Linear(in_features=200, out_features=200, bias=True)\n",
       "  (W): Linear(in_features=50, out_features=200, bias=True)\n",
       "  (relu1): ReLU()\n",
       "  (relu2): ReLU()\n",
       "  (fc1): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_model = RecurrentNetwork(max_len, 50, 100, 200, 5)\n",
    "rec_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  loss =  1.6116819381713867\n",
      "Epoch  10  loss =  1.2526684999465942\n",
      "Epoch  20  loss =  0.7527338862419128\n",
      "Epoch  30  loss =  0.3805331587791443\n",
      "Epoch  40  loss =  0.16208380460739136\n",
      "Epoch  50  loss =  0.05343855172395706\n",
      "Epoch  60  loss =  0.01836515963077545\n",
      "Epoch  70  loss =  0.00832908134907484\n",
      "Epoch  80  loss =  0.0047454689629375935\n",
      "Epoch  90  loss =  0.00325506622903049\n",
      "Epoch  100  loss =  0.0024645666126161814\n",
      "Epoch  110  loss =  0.0019869725219905376\n",
      "Epoch  120  loss =  0.0016608238220214844\n",
      "Epoch  130  loss =  0.0014193409588187933\n",
      "Epoch  140  loss =  0.0012315179919824004\n",
      "Epoch  150  loss =  0.001081392401829362\n",
      "Epoch  160  loss =  0.0009582837228663266\n",
      "Epoch  170  loss =  0.000855709018651396\n",
      "Epoch  180  loss =  0.0007696021348237991\n",
      "Epoch  190  loss =  0.0006962291663512588\n",
      "Epoch  200  loss =  0.0006329348543658853\n",
      "Epoch  210  loss =  0.0005777051555924118\n",
      "Epoch  220  loss =  0.0005297582829371095\n",
      "Epoch  230  loss =  0.00048750737914815545\n",
      "Epoch  240  loss =  0.0004502098308876157\n",
      "Epoch  250  loss =  0.000417050119722262\n",
      "Epoch  260  loss =  0.0003874471294693649\n",
      "Epoch  270  loss =  0.0003609031846281141\n",
      "Epoch  280  loss =  0.00033703265944495797\n",
      "Epoch  290  loss =  0.0003154160513076931\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rec_model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 300\n",
    "for epoch_cntr in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    trainO = rec_model(trainX)\n",
    "    loss = criterion(trainO, trainY)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch_cntr % 10 == 0:\n",
    "        print('Epoch ', epoch_cntr, ' loss = ', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9821428571428571\n"
     ]
    }
   ],
   "source": [
    "trainO = rec_model(trainX)\n",
    "trainP = torch.argmax(trainO, dim=1)\n",
    "\n",
    "print((trainY == trainP).sum().item() / trainY.size(0))\n",
    "\n",
    "testO = rec_model(testX)\n",
    "testP = torch.argmax(testO, dim=1)\n",
    "\n",
    "print((testY == testP).sum().item() / testY.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,   0,   0,   0,   0,  22, 311, 230, 109,  85]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sen=encode_sent('are you ready for dinner ')\n",
    "test_sen=test_sen.reshape((1,10))\n",
    "test_sent_output =rec_model(test_sen)\n",
    "test_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sent_pred = torch.argmax(test_sent_output, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-4.3561, -4.1964, -4.5877, -5.0749,  8.0769]],\n",
      "       grad_fn=<AddmmBackward>)\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "print(test_sent_output)\n",
    "print(test_sent_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
